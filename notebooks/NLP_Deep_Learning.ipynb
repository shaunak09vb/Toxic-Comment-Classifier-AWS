{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wcGq1dF0iWju",
        "z8KPqb7_vC59",
        "D8hAcigh3Sh3",
        "NDWRHguXxOF_",
        "CeScalvZDEdD",
        "7wv1IdwZk_qG",
        "bXx4IOYn4_XB",
        "kJJJs5txVS8S",
        "7HkAldq1IJcr"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaunak09vb/Toxic-Comment-Classifier-using-Deep-Learning/blob/main/NLP_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcGq1dF0iWju"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyTgJx85_MUt"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEXu-mePiK9T"
      },
      "source": [
        "import numpy as np, pandas as pd\n",
        "\n",
        "!pip install talos\n",
        "\n",
        "import re\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "import string\n",
        "from string import ascii_lowercase\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "import itertools\n",
        "import io\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from functools import reduce\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "import talos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8KPqb7_vC59"
      },
      "source": [
        "# Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96Z6cXAZiaAY"
      },
      "source": [
        "train=pd.read_csv('File_Path')    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTbZDdkc1lxE"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2_gDuPMvQHZ"
      },
      "source": [
        "test=pd.read_csv('File_Path')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vewUHsrQvcti"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8hAcigh3Sh3"
      },
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsOBGKJI4aHT"
      },
      "source": [
        "Checking for missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzAnL7ly1epE"
      },
      "source": [
        "train.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRc5Wyw24c62"
      },
      "source": [
        "test.isnull().any()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYZ93LlRbwRB"
      },
      "source": [
        "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\r\n",
        "y = train[labels].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDWRHguXxOF_"
      },
      "source": [
        "#Data Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z-4hq4jyhBw"
      },
      "source": [
        "## Text Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4k-vI8DbqiP"
      },
      "source": [
        "* Removing Characters in between Text\n",
        "* Removing Repeated Characters\n",
        "* Converting data to lower-case\n",
        "* Removing Numbers from the data\n",
        "* Remove Punctuation\n",
        "* Remove Whitespaces\n",
        "* Removing spaces in between words\n",
        "* Removing \"\\n\"\n",
        "* Remove Non-english characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jglnA7LEbpoA"
      },
      "source": [
        "RE_PATTERNS = {\n",
        "    ' american ':\n",
        "        [\n",
        "            'amerikan'\n",
        "        ],\n",
        "\n",
        "    ' adolf ':\n",
        "        [\n",
        "            'adolf'\n",
        "        ],\n",
        "\n",
        "\n",
        "    ' hitler ':\n",
        "        [\n",
        "            'hitler'\n",
        "        ],\n",
        "\n",
        "    ' fuck':\n",
        "        [\n",
        "            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n",
        "            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n",
        "            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n",
        "            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n",
        "            'feck ', ' fux ', 'f\\*\\*', 'f**k','fu*k',\n",
        "            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n",
        "        ],\n",
        "\n",
        "    ' ass ':\n",
        "        [\n",
        "            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$',\n",
        "            '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n",
        "            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s','a55', '@$$'\n",
        "        ],\n",
        "\n",
        "    ' ass hole ':\n",
        "        [\n",
        "            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole', 'a**hole'\n",
        "        ],\n",
        "\n",
        "    ' bitch ':\n",
        "        [\n",
        "            'b[w]*i[t]*ch', 'b!tch',\n",
        "            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
        "            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h', 'b!tch', 'bi+ch', 'l3itch'\n",
        "        ],\n",
        "\n",
        "    ' bastard ':\n",
        "        [\n",
        "            'ba[s|z]+t[e|a]+rd'\n",
        "        ],\n",
        "\n",
        "    ' trans gender':\n",
        "        [\n",
        "            'transgender'\n",
        "        ],\n",
        "\n",
        "    ' gay ':\n",
        "        [\n",
        "            'gay'\n",
        "        ],\n",
        "\n",
        "    ' cock ':\n",
        "        [\n",
        "            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n",
        "            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n",
        "        ],\n",
        "\n",
        "    ' dick ':\n",
        "        [\n",
        "            ' dick[^aeiou]', 'deek', 'd i c k', 'dik'\n",
        "        ],\n",
        "\n",
        "    ' suck ':\n",
        "        [\n",
        "            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n",
        "        ],\n",
        "\n",
        "    ' cunt ':\n",
        "        [\n",
        "            'cunt', 'c u n t'\n",
        "        ],\n",
        "\n",
        "    ' bull shit ':\n",
        "        [\n",
        "            'bullsh\\*t', 'bull\\$hit'\n",
        "        ],\n",
        "\n",
        "    ' homo sex ual':\n",
        "        [\n",
        "            'homosexual'\n",
        "        ],\n",
        "\n",
        "    ' jerk ':\n",
        "        [\n",
        "            'jerk'\n",
        "        ],\n",
        "\n",
        "    ' idiot ':\n",
        "        [\n",
        "            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots'\n",
        "                                                                                      'i d i o t'\n",
        "        ],\n",
        "\n",
        "    ' dumb ':\n",
        "        [\n",
        "            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n",
        "        ],\n",
        "\n",
        "    ' shit ':\n",
        "        [\n",
        "            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t', '$h1t'\n",
        "        ],\n",
        "\n",
        "    ' shit hole ':\n",
        "        [\n",
        "            'shythole'\n",
        "        ],\n",
        "\n",
        "    ' retard ':\n",
        "        [\n",
        "            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n",
        "        ],\n",
        "\n",
        "    ' rape ':\n",
        "        [\n",
        "            ' raped'\n",
        "        ],\n",
        "\n",
        "    ' dumb ass':\n",
        "        [\n",
        "            'dumbass', 'dubass'\n",
        "        ],\n",
        "\n",
        "    ' ass head':\n",
        "        [\n",
        "            'butthead'\n",
        "        ],\n",
        "\n",
        "    ' sex ':\n",
        "        [\n",
        "            'sexy', 's3x', 'sexuality'\n",
        "        ],\n",
        "\n",
        "\n",
        "    ' nigger ':\n",
        "        [\n",
        "            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n",
        "        ],\n",
        "\n",
        "    ' shut the fuck up':\n",
        "        [\n",
        "            'stfu', 'st*u'\n",
        "        ],\n",
        "\n",
        "    ' pussy ':\n",
        "        [\n",
        "            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses', 'p*ssy'\n",
        "        ],\n",
        "\n",
        "    ' faggot ':\n",
        "        [\n",
        "            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n",
        "            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n",
        "        ],\n",
        "\n",
        "    ' mother fucker':\n",
        "        [\n",
        "            ' motha ', ' motha f', ' mother f', 'motherucker',\n",
        "        ],\n",
        "\n",
        "    ' whore ':\n",
        "        [\n",
        "            'wh\\*\\*\\*', 'w h o r e'\n",
        "        ],\n",
        "    ' fucking ':\n",
        "        [\n",
        "            'f*$%-ing'\n",
        "        ],\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RetJ1vgswOG"
      },
      "source": [
        "def clean_text(text,remove_repeat_text=True, remove_patterns_text=True, is_lower=True):\n",
        "\n",
        "  if is_lower:\n",
        "    text=text.lower()\n",
        "    \n",
        "  if remove_patterns_text:\n",
        "    for target, patterns in RE_PATTERNS.items():\n",
        "      for pat in patterns:\n",
        "        text=str(text).replace(pat, target)\n",
        "\n",
        "  if remove_repeat_text:\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1', text) \n",
        "\n",
        "  text = str(text).replace(\"\\n\", \" \")\n",
        "  text = re.sub(r'[^\\w\\s]',' ',text)\n",
        "  text = re.sub('[0-9]',\"\",text)\n",
        "  text = re.sub(\" +\", \" \", text)\n",
        "  text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n",
        "  return text "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwyO_GcPuPE_"
      },
      "source": [
        "Cleaning Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cak-Q0fl0qyb"
      },
      "source": [
        "train['comment_text']=train['comment_text'].apply(lambda x: clean_text(x))\r\n",
        "train['comment_text'][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qfbjYXq8huL"
      },
      "source": [
        "Cleaning Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V02zaxGN8g-s"
      },
      "source": [
        "test['comment_text']=test['comment_text'].apply(lambda x: clean_text(x))\r\n",
        "test['comment_text'][1048]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqRVmH1FRWpL"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnhCAXkKUF9i"
      },
      "source": [
        "comments_train=train['comment_text']\n",
        "comments_test=test['comment_text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwhTNF3pW7Hp"
      },
      "source": [
        "comments_train=list(comments_train)\n",
        "comments_test=list(comments_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuWMc1xqRVqV"
      },
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi-vYUcPoi-a"
      },
      "source": [
        "def lemma(text, lemmatization=True):\n",
        "  output=\"\"\n",
        "  if lemmatization:\n",
        "    text=text.split(\" \")\n",
        "    for word in text:\n",
        "       word1 = wordnet_lemmatizer.lemmatize(word, pos = \"n\")\n",
        "       word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
        "       word3 = wordnet_lemmatizer.lemmatize(word2, pos = \"a\")\n",
        "       word4 = wordnet_lemmatizer.lemmatize(word3, pos = \"r\")\n",
        "       output=output + \" \" + word4\n",
        "  else:\n",
        "    output=text\n",
        "  \n",
        "  return str(output.strip()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX4DTjGJ-8Id"
      },
      "source": [
        "Lemmatizing Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRquFt30qWk0"
      },
      "source": [
        "lemmatized_train_data = [] \n",
        "\n",
        "for line in tqdm_notebook(comments_train, total=159571): \n",
        "    lemmatized_train_data.append(lemma(line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK9azHjSrbic"
      },
      "source": [
        "lemmatized_train_data[152458]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxRJbMBQ-oQ-"
      },
      "source": [
        "Lemmatizing Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4LZUmqy-vvD"
      },
      "source": [
        "lemmatized_test_data = [] \n",
        "\n",
        "for line in tqdm_notebook(comments_test, total=len(comments_test)): \n",
        "    lemmatized_test_data.append(lemma(line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZaP8BH2UG0N"
      },
      "source": [
        "## Stopwords Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNKjE4Vq8kpG"
      },
      "source": [
        "stopword_list=STOP_WORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_oqV08mMWmk"
      },
      "source": [
        "Adding Single and Dual to STOP_WORDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go7RVS52I1US"
      },
      "source": [
        "def iter_all_strings():\n",
        "    for size in itertools.count(1):\n",
        "        for s in itertools.product(ascii_lowercase, repeat=size):\n",
        "            yield \"\".join(s)\n",
        "\n",
        "dual_alpha_list=[]\n",
        "for s in iter_all_strings():\n",
        "    dual_alpha_list.append(s)\n",
        "    if s == 'zz':\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_x3bQ4MIGVK"
      },
      "source": [
        "dual_alpha_list.remove('i')\n",
        "dual_alpha_list.remove('a')\n",
        "dual_alpha_list.remove('am')\n",
        "dual_alpha_list.remove('an')\n",
        "dual_alpha_list.remove('as')\n",
        "dual_alpha_list.remove('at')\n",
        "dual_alpha_list.remove('be')\n",
        "dual_alpha_list.remove('by')\n",
        "dual_alpha_list.remove('do')\n",
        "dual_alpha_list.remove('go')\n",
        "dual_alpha_list.remove('he')\n",
        "dual_alpha_list.remove('hi')\n",
        "dual_alpha_list.remove('if')\n",
        "dual_alpha_list.remove('is')\n",
        "dual_alpha_list.remove('in')\n",
        "dual_alpha_list.remove('me')\n",
        "dual_alpha_list.remove('my')\n",
        "dual_alpha_list.remove('no')\n",
        "dual_alpha_list.remove('of')\n",
        "dual_alpha_list.remove('on')\n",
        "dual_alpha_list.remove('or')\n",
        "dual_alpha_list.remove('ok')\n",
        "dual_alpha_list.remove('so')\n",
        "dual_alpha_list.remove('to')\n",
        "dual_alpha_list.remove('up')\n",
        "dual_alpha_list.remove('us')\n",
        "dual_alpha_list.remove('we')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H11kkMtXMyct"
      },
      "source": [
        "for letter in dual_alpha_list:\n",
        "    stopword_list.add(letter)\n",
        "print(\"Done!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti67XuLCNnsL"
      },
      "source": [
        "Checking for other words that we may need in STOP_WORDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4lBqjcBaDVK"
      },
      "source": [
        "def search_stopwords(data, search_stop=True):\n",
        "  output=\"\"\n",
        "  if search_stop:\n",
        "    data=data.split(\" \")\n",
        "    for word in data:\n",
        "      if not word in stopword_list:\n",
        "        output=output+\" \"+word \n",
        "  else:\n",
        "    output=data\n",
        "\n",
        "  return str(output.strip())  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn21RMeIbQti"
      },
      "source": [
        "potential_stopwords = [] \n",
        "\n",
        "for line in tqdm_notebook(lemmatized_train_data, total=159571): \n",
        "    potential_stopwords.append(search_stopwords(line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4Yp_YRoap6Q"
      },
      "source": [
        "len(potential_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFSkmGjAZuR3"
      },
      "source": [
        "Combining all the sentences in the list into a single string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpmze-Q7BrK0"
      },
      "source": [
        "def string_combine_a(stopword):\n",
        "  final_a=\"\"\n",
        "  for item in range(39893):\n",
        "    final_a=final_a+\" \"+stopword[item]\n",
        "  return final_a\n",
        "\n",
        "def string_combine_b(stopword):\n",
        "  final_b=\"\"\n",
        "  for item in range(39893,79785):\n",
        "    final_b=final_b+\" \"+stopword[item]\n",
        "  return final_b\n",
        "\n",
        "def string_combine_c(stopword):\n",
        "  final_c=\"\"\n",
        "  for item in range(79785,119678):\n",
        "    final_c=final_c+\" \"+stopword[item]\n",
        "  return final_c\n",
        "\n",
        "def string_combine_d(stopword):\n",
        "  final_d=\"\"\n",
        "  for item in range(119678,159571):\n",
        "    final_d=final_d+\" \"+stopword[item]\n",
        "  return final_d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqBc4rOahmAz"
      },
      "source": [
        "total_string_potential_a=string_combine_a(potential_stopwords)\n",
        "total_string_potential_b=string_combine_b(potential_stopwords)\n",
        "total_string_potential_c=string_combine_c(potential_stopwords)\n",
        "total_string_potential_d=string_combine_d(potential_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbEm3D8txMSH"
      },
      "source": [
        "Counting the number of words in each of the 4 strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_hhbMS_xR6t"
      },
      "source": [
        "def word_count(str):\n",
        "    counts = dict()\n",
        "    words = str.split()\n",
        "\n",
        "    for word in words:\n",
        "        if word in counts:\n",
        "            counts[word] += 1\n",
        "        else:\n",
        "            counts[word] = 1\n",
        "\n",
        "    return counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsh0BuT9xSDg"
      },
      "source": [
        "total_string_potential_a_dict=word_count(total_string_potential_a)\n",
        "total_string_potential_b_dict=word_count(total_string_potential_b)\n",
        "total_string_potential_c_dict=word_count(total_string_potential_c)\n",
        "total_string_potential_d_dict=word_count(total_string_potential_d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shmHkLSk3Xxp"
      },
      "source": [
        "Converting Dictionaries to Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSJXHnv6y9T0"
      },
      "source": [
        "total_string_potential_a_df = pd.DataFrame(list(total_string_potential_a_dict.items()),columns = ['Word','Count'])\n",
        "total_string_potential_b_df = pd.DataFrame(list(total_string_potential_b_dict.items()),columns = ['Word','Count'])\n",
        "total_string_potential_c_df = pd.DataFrame(list(total_string_potential_c_dict.items()),columns = ['Word','Count'])\n",
        "total_string_potential_d_df = pd.DataFrame(list(total_string_potential_d_dict.items()),columns = ['Word','Count'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjWO0vj2yaW7"
      },
      "source": [
        "Getting Dataframe output in descending order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzThNOjA2pjx"
      },
      "source": [
        "top50_potential_stopwords_a=total_string_potential_a_df.sort_values(by=['Count'],ascending=False).head(50)\n",
        "top50_potential_stopwords_b=total_string_potential_b_df.sort_values(by=['Count'],ascending=False).head(50)\n",
        "top50_potential_stopwords_c=total_string_potential_c_df.sort_values(by=['Count'],ascending=False).head(50)\n",
        "top50_potential_stopwords_d=total_string_potential_d_df.sort_values(by=['Count'],ascending=False).head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1d8reN9CaZ8"
      },
      "source": [
        "Looking for common terms in all top 50 dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKUuEbHvCiYG"
      },
      "source": [
        "common_potential_stopwords=list(reduce(set.intersection,map(set,[top50_potential_stopwords_a.Word,top50_potential_stopwords_b.Word,top50_potential_stopwords_c.Word,top50_potential_stopwords_d.Word])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkQme4SIEGBa"
      },
      "source": [
        "print(common_potential_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX3lIQj0l36i"
      },
      "source": [
        "Retaining certain words and removing others from the above list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zacbW5ASjN2r"
      },
      "source": [
        "potential_stopwords=['editor', 'reference', 'thank', 'work','find', 'good', 'know', 'like', 'look', 'thing', 'want', 'time', 'list', 'section','wikipedia', 'doe', 'add','new', 'try', 'think', 'write','use', 'user', 'way', 'page']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-yjjvj3LpZs"
      },
      "source": [
        "Adding above retrived words into the stopwords list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73PbxjxbLvz9"
      },
      "source": [
        "for word in potential_stopwords:\n",
        "    stopword_list.add(word)\n",
        "print(\"Done!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "101v0iZjaNLR"
      },
      "source": [
        "Removing Stopwords from Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRSga8PKsktA"
      },
      "source": [
        "def remove_stopwords(text, remove_stop=True):\n",
        "  output = \"\"\n",
        "  if remove_stop:\n",
        "    text=text.split(\" \")\n",
        "    for word in text:\n",
        "      if word not in stopword_list:\n",
        "        output=output + \" \" + word\n",
        "  else :\n",
        "    output=text\n",
        "\n",
        "  return str(output.strip())      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-hItiV7skoV"
      },
      "source": [
        "processed_train_data = [] \n",
        "\n",
        "for line in tqdm_notebook(lemmatized_train_data, total=159571): \n",
        "    processed_train_data.append(remove_stopwords(line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a29qOSPA-rbS"
      },
      "source": [
        "processed_train_data[152458]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqXHFaSC-Bkf"
      },
      "source": [
        "Removing Stopwords from Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ_lUOej-H_b"
      },
      "source": [
        "processed_test_data = [] \n",
        "\n",
        "for line in tqdm_notebook(lemmatized_test_data, total=153164): \n",
        "    processed_test_data.append(remove_stopwords(line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeScalvZDEdD"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkK1P-CdX_0N"
      },
      "source": [
        "max_features=100000      \r\n",
        "maxpadlen = 200          \r\n",
        "val_split = 0.2      \r\n",
        "embedding_dim_fasttext = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIeovLIr6aAo"
      },
      "source": [
        "Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaA52PlK4xnV"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(processed_train_data))\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(processed_train_data)\n",
        "list_tokenized_test = tokenizer.texts_to_sequences(processed_test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkSXj5sT7jbR"
      },
      "source": [
        "word_index=tokenizer.word_index\n",
        "print(\"Words in Vocabulary: \",len(word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfDwmzPj8TJf"
      },
      "source": [
        "Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEmacO337twa"
      },
      "source": [
        "X_t=pad_sequences(list_tokenized_train, maxlen=maxpadlen, padding = 'post')\n",
        "X_te=pad_sequences(list_tokenized_test, maxlen=maxpadlen, padding = 'post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6dAeWAnM4RL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "675973f5-74c8-4cf1-c0c4-0408f85ebf48"
      },
      "source": [
        "print('Tokenized sentences: \\n', X_t[10])\r\n",
        "print('One hot label: \\n', y[10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized sentences: \n",
            " [  116   578    11 32199   239   192    11 32199   239    90    11   579\n",
            "    11   116   366   578     1  1082   116   338  5356   116   119    11\n",
            "   387   269   366   578    11     1  1341   116    11   387     2    32\n",
            "   116   578   192   116   172    46    84   579   116   578    11     2\n",
            "   488   105    10  1087   403  1073    11  2401   489    36   116    11\n",
            "   192   407   366     9   255   192   242   150   109    18    19    29\n",
            "   172    77    19     3   257  4540     4    11 32199   239   192    11\n",
            " 32199   239    90   171   387   312    68   579    34    44    77   423\n",
            "  1550    34   171     8   579  1268    77  1638   134    10   134  4671\n",
            "   134    94    44   565  1255    17    77  1338   118   134  1577    77\n",
            "  1134     4   507    77   434    31   171    68    34   168  2207   449\n",
            "    31   315  1529    70   172   236   150   116    31    31    11    77\n",
            "    31   116    11    77    31    77    31   192   171    46    84   579\n",
            "     4    31   171   192    80    10   862  3126    11     9   255    31\n",
            "   242   150   109    18    11    77   124    67   434   116    11     9\n",
            "   357    19    29   172    77    19     3   257     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n",
            "One hot label: \n",
            " [0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtZD1j9BCrsG"
      },
      "source": [
        "indices = np.arange(X_t.shape[0])\r\n",
        "np.random.shuffle(indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k51hkGqOnZLB"
      },
      "source": [
        "X_t = X_t[indices]\r\n",
        "labels = y[indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWn-SkJCCswd"
      },
      "source": [
        "### Splitting data into Training and Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5hOSGJVb7o4"
      },
      "source": [
        "num_validation_samples = int(val_split*X_t.shape[0])\r\n",
        "x_train = X_t[: -num_validation_samples]\r\n",
        "y_train = labels[: -num_validation_samples]\r\n",
        "x_val = X_t[-num_validation_samples: ]\r\n",
        "y_val = labels[-num_validation_samples: ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHyYFZPDcEmK"
      },
      "source": [
        "print('Number of entries in each category:')\r\n",
        "print('training: ', y_train.sum(axis=0))\r\n",
        "print('validation: ', y_val.sum(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL7Mv1Z6D8_w"
      },
      "source": [
        "### Importing Fast Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsvNLXZ6DlWS"
      },
      "source": [
        "embeddings_index_fasttext = {}\r\n",
        "f = open('File_Path', encoding='utf8')\r\n",
        "for line in f:\r\n",
        "    values = line.split()\r\n",
        "    word = values[0]\r\n",
        "    embeddings_index_fasttext[word] = np.asarray(values[1:], dtype='float32')\r\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpkSOB6iEFCB"
      },
      "source": [
        "embedding_matrix_fasttext = np.random.random((len(word_index) + 1, embedding_dim_fasttext))\r\n",
        "for word, i in word_index.items():\r\n",
        "    embedding_vector = embeddings_index_fasttext.get(word)\r\n",
        "    if embedding_vector is not None:\r\n",
        "        embedding_matrix_fasttext[i] = embedding_vector\r\n",
        "print(\" Completed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OoZ7fTpPWhi"
      },
      "source": [
        "### Creating Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dwzg_WX32OG3"
      },
      "source": [
        "#### Talos Grid Search  for LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZdik3-jGkye"
      },
      "source": [
        "def toxic_classifier(x_train,y_train,x_val,y_val,params):\r\n",
        "\r\n",
        "  inp=Input(shape=(maxpadlen, ),dtype='int32')\r\n",
        "\r\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\r\n",
        "                           embedding_dim_fasttext,\r\n",
        "                           weights = [embedding_matrix_fasttext],\r\n",
        "                           input_length = maxpadlen,\r\n",
        "                           trainable=False,\r\n",
        "                           name = 'embeddings')\r\n",
        "  embedded_sequences = embedding_layer(inp)\r\n",
        "\r\n",
        "  x = LSTM(params['output_count_lstm'], return_sequences=True,name='lstm_layer')(embedded_sequences)\r\n",
        "  \r\n",
        "  x = GlobalMaxPool1D()(x)\r\n",
        "  \r\n",
        "  x = Dropout(params['dropout'])(x)\r\n",
        "  \r\n",
        "  x = Dense(params['output_count_dense'], activation=params['activation'], kernel_initializer='he_uniform')(x)\r\n",
        "  \r\n",
        "  x = Dropout(params['dropout'])(x)\r\n",
        "  \r\n",
        "  preds = Dense(6, activation=params['last_activation'], kernel_initializer='glorot_uniform')(x)\r\n",
        "\r\n",
        "  model = Model(inputs=inp, outputs=preds)\r\n",
        "\r\n",
        "  model.compile(loss=params['loss'], optimizer=params['optimizer'], metrics=['accuracy'])\r\n",
        "\r\n",
        "  model_info=model.fit(x_train,y_train, epochs=params['epochs'], batch_size=params['batch_size'],  validation_data=(x_val, y_val))\r\n",
        "\r\n",
        "  return model_info, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6AtSibHGoTU"
      },
      "source": [
        "p={\r\n",
        "    'output_count_lstm': [40,50,60],\r\n",
        "    'output_count_dense': [30,40,50],\r\n",
        "    'batch_size': [32],\r\n",
        "    'epochs':[2],\r\n",
        "    'optimizer':['adam'],\r\n",
        "    'activation':['relu'],\r\n",
        "    'last_activation': ['sigmoid'],\r\n",
        "    'dropout':[0.1,0.2],\r\n",
        "    'loss': ['binary_crossentropy']   \r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAnaHxh1GpFi"
      },
      "source": [
        "scan_results = talos.Scan(x=x_train,\r\n",
        "               y=y_train,\r\n",
        "               x_val=x_val,\r\n",
        "               y_val=y_val,\r\n",
        "               model=toxic_classifier,\r\n",
        "               params=p,\r\n",
        "               experiment_name='tcc',\r\n",
        "               print_params=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXl1ZQSWtdYK"
      },
      "source": [
        "model_id = scan_results.data['val_accuracy'].astype('float').argmax()\r\n",
        "model_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yict0QyGxV7"
      },
      "source": [
        "analyze_object = talos.Analyze(scan_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3d78iYYG0I3"
      },
      "source": [
        "analyze_object.best_params('val_accuracy', ['accuracy', 'loss', 'val_loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBlqpfIhG2xq"
      },
      "source": [
        "analyze_object.plot_line('val_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ8Xr0tiG4ry"
      },
      "source": [
        "analyze_object.plot_line('accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV4ierVz7J02"
      },
      "source": [
        "#### Talos Grid Search  for LSTM-CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH7jcbAs7MXv"
      },
      "source": [
        "def toxic_classifier(x_train,y_train,x_val,y_val,params):\r\n",
        "\r\n",
        "  inp=Input(shape=(maxpadlen, ),dtype='int32')\r\n",
        "\r\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\r\n",
        "                           embedding_dim_fasttext,\r\n",
        "                           weights = [embedding_matrix_fasttext],\r\n",
        "                           input_length = maxpadlen,\r\n",
        "                           trainable=False,\r\n",
        "                           name = 'embeddings')\r\n",
        "  embedded_sequences = embedding_layer(inp)\r\n",
        "\r\n",
        "  x = LSTM(params['output_count_lstm'], return_sequences=True,name='lstm_layer')(embedded_sequences)\r\n",
        "\r\n",
        "  x = Conv1D(filters=params['filters'], kernel_size=params['kernel_size'], padding='same', activation='relu', kernel_initializer='he_uniform')(x)\r\n",
        "\r\n",
        "  x = MaxPooling1D(params['pool_size'])(x)\r\n",
        "  \r\n",
        "  x = GlobalMaxPool1D()(x)\r\n",
        "  \r\n",
        "  x = BatchNormalization()(x)\r\n",
        "  \r\n",
        "  x = Dense(params['output_1_count_dense'], activation=params['activation'], kernel_initializer='he_uniform')(x)\r\n",
        "  \r\n",
        "  x = Dropout(params['dropout'])(x)\r\n",
        "\r\n",
        "  x = Dense(params['output_2_count_dense'], activation=params['activation'], kernel_initializer='he_uniform')(x)\r\n",
        "  \r\n",
        "  x = Dropout(params['dropout'])(x)\r\n",
        "  \r\n",
        "  preds = Dense(6, activation=params['last_activation'], kernel_initializer='glorot_uniform')(x)\r\n",
        "\r\n",
        "  model = Model(inputs=inp, outputs=preds)\r\n",
        "\r\n",
        "  model.compile(loss=params['loss'], optimizer=params['optimizer'], metrics=['accuracy'])\r\n",
        "\r\n",
        "  model_info=model.fit(x_train,y_train, epochs=params['epochs'], batch_size=params['batch_size'],  validation_data=(x_val, y_val))\r\n",
        "\r\n",
        "  return model_info, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ru7Tkq67QoO"
      },
      "source": [
        "p={\r\n",
        "    'output_count_lstm': [50,60],\r\n",
        "    'output_1_count_dense': [40,50],\r\n",
        "    'output_2_count_dense': [30,40],\r\n",
        "    'filters' : [64],\r\n",
        "    'kernel_size' : [3],\r\n",
        "    'batch_size': [32],\r\n",
        "    'pool_size': [3],\r\n",
        "    'epochs':[2],\r\n",
        "    'optimizer':['adam'],\r\n",
        "    'activation':['relu'],\r\n",
        "    'last_activation': ['sigmoid'],\r\n",
        "    'dropout':[0.1,0.2],\r\n",
        "    'loss': ['binary_crossentropy']   \r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l3M27FU7QiG"
      },
      "source": [
        "scan_results = talos.Scan(x=x_train,\r\n",
        "               y=y_train,\r\n",
        "               x_val=x_val,\r\n",
        "               y_val=y_val,\r\n",
        "               model=toxic_classifier,\r\n",
        "               params=p,\r\n",
        "               experiment_name='tcc',\r\n",
        "               print_params=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQU9iqMM_IT2"
      },
      "source": [
        "model_id = scan_results.data['val_accuracy'].astype('float').argmax()\r\n",
        "model_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OiN965L_JOz"
      },
      "source": [
        "scan_results.data[8:9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbIfeqlH_JK-"
      },
      "source": [
        "analyze_object = talos.Analyze(scan_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMfSKDje_JHw"
      },
      "source": [
        "analyze_object.best_params('val_accuracy', ['accuracy', 'loss', 'val_loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K40v1m_N_Rgo"
      },
      "source": [
        "analyze_object.plot_line('val_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z50AydTs_RdW"
      },
      "source": [
        "analyze_object.plot_line('accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHrPeYI_Giky"
      },
      "source": [
        "#### Training Model with Best Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIRoKKp9mzQz"
      },
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x04BXTCCdsV"
      },
      "source": [
        "inp=Input(shape=(maxpadlen, ),dtype='int32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-fPTQKHFF61"
      },
      "source": [
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                           embedding_dim_fasttext,\n",
        "                           weights = [embedding_matrix_fasttext],\n",
        "                           input_length = maxpadlen,\n",
        "                           trainable=False,\n",
        "                           name = 'embeddings')\n",
        "embedded_sequences = embedding_layer(inp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAF1Ui22FmHX"
      },
      "source": [
        "x = LSTM(40, return_sequences=True,name='lstm_layer')(embedded_sequences)\r\n",
        "x = GlobalMaxPool1D()(x)\r\n",
        "x = Dropout(0.1)(x)\r\n",
        "x = Dense(30, activation=\"relu\", kernel_initializer='he_uniform')(x)\r\n",
        "x = Dropout(0.1)(x)\r\n",
        "preds = Dense(6, activation=\"sigmoid\", kernel_initializer='glorot_uniform')(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kigFyK4cHHrv"
      },
      "source": [
        "model_1 = Model(inputs=inp, outputs=preds)\n",
        "model_1.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_qJGa25HVxi"
      },
      "source": [
        "model_1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSqW0thULSae"
      },
      "source": [
        "model_info_1=model_1.fit(x_train,y_train, epochs=2, batch_size=32,  validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q63N5a-Rm0zp"
      },
      "source": [
        "LSTM-CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq7eVkGSLKhm"
      },
      "source": [
        "inp=Input(shape=(maxpadlen, ),dtype='int32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QLeM4ieLLEL"
      },
      "source": [
        "embedding_layer = Embedding(len(word_index) + 1,\r\n",
        "                           embedding_dim_fasttext,\r\n",
        "                           weights = [embedding_matrix_fasttext],\r\n",
        "                           input_length = maxpadlen,\r\n",
        "                           trainable=False,\r\n",
        "                           name = 'embeddings')\r\n",
        "embedded_sequences = embedding_layer(inp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKK_321BdT-o"
      },
      "source": [
        "x = LSTM(50, return_sequences=True,name='lstm_layer')(embedded_sequences)\r\n",
        "x = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_uniform')(x)\r\n",
        "x = MaxPooling1D(3)(x)\r\n",
        "x = GlobalMaxPool1D()(x)\r\n",
        "x = BatchNormalization()(x)\r\n",
        "x = Dense(40, activation=\"relu\", kernel_initializer='he_uniform')(x)\r\n",
        "x = Dropout(0.2)(x)\r\n",
        "x = Dense(30, activation=\"relu\", kernel_initializer='he_uniform')(x)\r\n",
        "x = Dropout(0.2)(x)\r\n",
        "preds = Dense(6, activation=\"sigmoid\", kernel_initializer='glorot_uniform')(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h965XoLoLO5a"
      },
      "source": [
        "model_2 = Model(inputs=inp, outputs=preds)\r\n",
        "model_2.compile(loss='binary_crossentropy',\r\n",
        "                  optimizer='adam',\r\n",
        "                  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_UNnc0ELQbL"
      },
      "source": [
        "model_2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc-HX52zHNg9"
      },
      "source": [
        "model_info_2=model_2.fit(x_train,y_train, epochs=2, batch_size=32,  validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL2OV0YiB3sp"
      },
      "source": [
        "## Plotting Graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK9KghRBLlq9"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qKSCDX-KjSX"
      },
      "source": [
        "loss = model_info_1.history['loss']\r\n",
        "val_loss = model_info_1.history['val_loss']\r\n",
        "\r\n",
        "epochs = range(1, len(loss)+1)\r\n",
        "\r\n",
        "plt.plot(epochs, loss, label='Training loss')\r\n",
        "plt.plot(epochs, val_loss, label='Validation loss')\r\n",
        "plt.title('Training and Validation loss')\r\n",
        "plt.xlabel('Epochs')\r\n",
        "plt.ylabel('Loss')\r\n",
        "plt.legend()\r\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zio5y-3yRn-6"
      },
      "source": [
        "accuracy = model_info_1.history['accuracy']\r\n",
        "val_accuracy = model_info_1.history['val_accuracy']\r\n",
        "\r\n",
        "plt.plot(epochs, accuracy, label='Training accuracy')\r\n",
        "plt.plot(epochs, val_accuracy, label='Validation accuracy')\r\n",
        "plt.title('Training and validation accuracy')\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.xlabel('Epochs')\r\n",
        "plt.legend()\r\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKNnZW0TLoO_"
      },
      "source": [
        "### LSTM-CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOrZB4k0LqQx"
      },
      "source": [
        "loss = model_info_2.history['loss']\r\n",
        "val_loss = model_info_2.history['val_loss']\r\n",
        "\r\n",
        "epochs = range(1, len(loss)+1)\r\n",
        "\r\n",
        "plt.plot(epochs, loss, label='Training loss')\r\n",
        "plt.plot(epochs, val_loss, label='Validation loss')\r\n",
        "plt.title('Training and Validation loss')\r\n",
        "plt.xlabel('Epochs')\r\n",
        "plt.ylabel('Loss')\r\n",
        "plt.legend()\r\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67M15ZndRt97"
      },
      "source": [
        "accuracy = model_info_2.history['accuracy']\r\n",
        "val_accuracy = model_info_2.history['val_accuracy']\r\n",
        "\r\n",
        "plt.plot(epochs, accuracy, label='Training accuracy')\r\n",
        "plt.plot(epochs, val_accuracy, label='Validation accuracy')\r\n",
        "plt.title('Training and validation accuracy')\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.xlabel('Epochs')\r\n",
        "plt.legend()\r\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wv1IdwZk_qG"
      },
      "source": [
        "# Saving the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eisFdjOUlBbM"
      },
      "source": [
        "model_1.save(filepath=\"File_Path\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNNf_Fj2MLgx"
      },
      "source": [
        "model_2.save(filepath=\"File_Path\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXx4IOYn4_XB"
      },
      "source": [
        "# Loading Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huVktHqRMSGc"
      },
      "source": [
        "loaded_model_1 = keras.models.load_model(filepath=\"File_Path\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulj64J7p4YKB"
      },
      "source": [
        "loaded_model_2 = keras.models.load_model(filepath=\"File_Path\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJJJs5txVS8S"
      },
      "source": [
        "# Generating the Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBk_wYFWMY82"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUL3S8xiCYxU"
      },
      "source": [
        "test_values_1 = loaded_model_1.predict([X_te], batch_size=1, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FSvu4hpyU4r"
      },
      "source": [
        "sample_submission = pd.read_csv('File_Path')\r\n",
        "test_values_1=pd.DataFrame(test_values_1,columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\r\n",
        "submission = pd.DataFrame(sample_submission[\"id\"])\r\n",
        "combined_submission=pd.concat([submission,test_values_1],axis=1)\r\n",
        "combined_submission.to_csv('File_Path', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xw4kR9PMyVy"
      },
      "source": [
        "### LSTM-CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zmj2YixM0dH"
      },
      "source": [
        "test_values_2 = loaded_model_2.predict([X_te], batch_size=1, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsUf3_HxM0Yw"
      },
      "source": [
        "sample_submission = pd.read_csv('File_Path')\r\n",
        "test_values_2=pd.DataFrame(test_values_2,columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\r\n",
        "submission = pd.DataFrame(sample_submission[\"id\"])\r\n",
        "combined_submission=pd.concat([submission,test_values_2],axis=1)\r\n",
        "combined_submission.to_csv('File_Path', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HkAldq1IJcr"
      },
      "source": [
        "# Testing the Created Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPneHbuzIIxj"
      },
      "source": [
        "def toxicity_level(string):\r\n",
        "    new_string = [string]\r\n",
        "    new_string = tokenizer.texts_to_sequences(new_string)\r\n",
        "    new_string = pad_sequences(new_string, maxlen=maxpadlen, padding='post')\r\n",
        "    \r\n",
        "    prediction = model.predict(new_string) #(Change to model_1 or model_2 depending on the preference of model type|| Model 1: LSTM, Model 2:LSTM-CNN)\r\n",
        "    \r\n",
        "    print(\"Toxicity levels for '{}':\".format(string))\r\n",
        "    print('Toxic:         {:.0%}'.format(prediction[0][0]))\r\n",
        "    print('Severe Toxic:  {:.0%}'.format(prediction[0][1]))\r\n",
        "    print('Obscene:       {:.0%}'.format(prediction[0][2]))\r\n",
        "    print('Threat:        {:.0%}'.format(prediction[0][3]))\r\n",
        "    print('Insult:        {:.0%}'.format(prediction[0][4]))\r\n",
        "    print('Identity Hate: {:.0%}'.format(prediction[0][5]))\r\n",
        "    print()\r\n",
        "    \r\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxWkAax7JLF1"
      },
      "source": [
        "toxicity_level('go jump off a bridge jerk')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLGZpQ1KIpFG"
      },
      "source": [
        "toxicity_level('i will kill you')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be1lw-liIpfe"
      },
      "source": [
        "toxicity_level('have a nice day')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ-_6XzxIxxT"
      },
      "source": [
        "toxicity_level('fuck ofF!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s_5EQEFJD3G"
      },
      "source": [
        "toxicity_level('Hello, How are you?')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apOGOBU3uZxl"
      },
      "source": [
        "toxicity_level('get the fuck away from me @sshole!!')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}